<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0035)http://www.cs.vu.nl/das4/jobs.shtml -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <title>DAS-4: Distributed ASCI Supercomputer 4</title>
    <link rel="stylesheet" type="text/css" href="./DAS-4  Distributed ASCI Supercomputer 4_files/das4.css" media="all">
    <!--[if IE]>
    <style type="text/css" media="all">.borderitem {border-style:solid;}</style>
    <![endif]-->
    <script src="./DAS-4  Distributed ASCI Supercomputer 4_files/SpryMenuBar.js" type="text/javascript"></script>
    <link href="./DAS-4  Distributed ASCI Supercomputer 4_files/SpryMenuBarHorizontal.css" rel="stylesheet" type="text/css">
</head>

<body>
<div id="Spacer">
    <div id="Background_Layer">
                <div id="Header">
            <div id="Linkbar">
	        <br>
                <center><font size="4"><a href="http://www.asci.tudelft.nl/">ASCI Homepage</a></font></center>
            </div>
            <div id="Menu">
                <ul id="MainMenu" class="MenuBarHorizontal">
                <li><a href="http://www.cs.vu.nl/das4/home.shtml">Home</a></li>
                <li><a class="MenuBarItemSubmenu" href="http://www.cs.vu.nl/das4/about.shtml">About DAS-4</a>
                    <ul>
                    <li><a href="http://www.cs.vu.nl/das4/clusters.shtml">Cluster Sites</a></li>
                    <li><a href="http://www.cs.vu.nl/das4/connectivity.shtml">Connectivity</a></li>
                    <li><a href="http://www.cs.vu.nl/das4/steering.shtml">Steering group</a></li>
                    <li><a href="http://www.cs.vu.nl/das4/history.shtml">History</a></li>
                    <li><a href="http://www.cs.vu.nl/das4/name.shtml">Name</a></li>
                    </ul>
                </li>
                <li><a class="MenuBarItemSubmenu" href="http://www.cs.vu.nl/das4/research.shtml">Research</a>
                    <ul>
                    <li><a href="http://www.cs.vu.nl/das4/achievements.shtml">Achievements</a></li>
                    <li><a href="http://www.cs.vu.nl/das4/phd.shtml">PhD theses</a></li>
                    </ul>
                </li>
                <li><a class="MenuBarItemSubmenu" href="http://www.cs.vu.nl/das4/users.shtml">Users</a>
                    <ul>
                    <li><a href="http://www.cs.vu.nl/das4/accounts.shtml">Accounts</a></li>
                    <li><a href="http://www.cs.vu.nl/das4/usage.shtml">Usage Policy</a></li>
                    <li><a href="./DAS-4  Distributed ASCI Supercomputer 4_files/DAS-4  Distributed ASCI Supercomputer 4.html">Job Execution</a></li>
                    <li><a href="http://www.cs.vu.nl/das4/network.shtml">Networks</a></li>
                    <li><a href="http://www.cs.vu.nl/das4/special.shtml">Special Nodes</a></li>
                    <li><a href="http://www.cs.vu.nl/das4/gpu.shtml">GPUs</a></li>
                    <li><a href="http://www.cs.vu.nl/das4/xeonphi.shtml">Xeon Phi</a></li>
                    <li><a href="http://www.cs.vu.nl/das4/hadoop.shtml">Hadoop</a></li>
                    <li><a href="http://www.cs.vu.nl/das4/pdu.shtml">Power monitoring</a></li>
                    </ul>
                </li>
                <li><a href="http://www.cs.vu.nl/das4/sponsors.shtml">Sponsors</a></li>
                </ul>
            </div>
        </div>



<div id="Middle">
    <div id="Announcements-Header">
    <h2>Announcements</h2>
</div>
<div id="Announcements">
    <div id="Announcements-Text">

<h3>6 Jul 2015</h3>
<p>
<a href="http://www.cs.vu.nl/das5">DAS-5</a> is now fully operational!

</p><h3>28 Oct 2014</h3>
<p>
The <a href="http://www.cs.vu.nl/das4/hadoop.shtml">Hadoop</a> setup on DAS-4/VU has been updated
to version 2.5.0.

</p><h3>30 Jan 2014</h3>
<p>
The Intel OpenCL package for Intel CPU's and <a href="http://www.cs.vu.nl/das4/xeonphi.shtml">Xeon Phi</a>
has been updated to version 3.2.1.

</p><h3>3 Sep 2013</h3>
<p>
The Nvidia CUDA development kit has been updated to version 5.5.

</p><h3>25 April 2013</h3>
<a href="http://www.asci.tudelft.nl/pages/about-asci/das/das-workshop-2013.php">Slides</a>
of the DAS-4 workshop presentations are now available.

<h3>14 Jan 2013</h3>
<p>
DAS-4/VU now has 8 new nodes with latest <a href="http://www.cs.vu.nl/das4/special.shtml">Nvidia K20 GPU</a>.

    </p></div>
</div>

    <div id="Content">

<h1>DAS-4 Job Execution</h1>
<hr>

<h2>General</h2>

<p>
Programs are started on the DAS-4 compute nodes using the
<a href="http://gridscheduler.sourceforge.net/documentation.html">Grid Engine (SGE)</a>
batch queueing system. The SGE system reserves the requested number of
nodes for the duration of a program run. It is also possible to reserve a
number of hosts in advance, terminate running jobs, or query the status of
current jobs. A full <a href="http://gridscheduler.sourceforge.net/howto/GridEngineHowto.html">
documentation set</a> is available.

</p><h2>Job time policy on DAS-4</h2>

<p>
The default run time for SGE jobs on DAS-4 is 15 minutes,
which is also the <strong>maximum</strong> for jobs on DAS-4
during working hours.  We do not want people to monopolize
the clusters for a long time, since that makes interactively
running short jobs on many or all nodes practically impossible.

</p><p>
During daytime, DAS-4 is specifically <strong>NOT</strong> a cluster
for doing production work.  It is meant for people doing experimental
research on parallel and distributed programming.
Only during the night and in the weekend, when DAS-4 is
regularly idle, it is allowed to run long jobs.
In all other cases, you will first have to ask permission
in advance from
<a href="mailto:das-sysadm@cs.vu.nl">das-sysadm@cs.vu.nl</a>
to make sure you are not causing too much trouble for other users.
More information is on the <a href="http://www.cs.vu.nl/das4/usage.shtml">DAS-4 Usage Policy</a>
page.

</p><h2>SGE</h2>

<p>
Both SGE and prun on DAS-4 are imported into the development environment
of the current login session (by setting the PATH and
appropriate other environment variables) using the command<br>
<tt>module load prun</tt>

</p><p>
The most often used SGE commands are:
</p><ul>
<li><em>qsub</em>: submit a new job</li>
<li><em>qstat</em>: ask status about current jobs</li>
<li><em>qdel</em>: delete a queued job</li>
</ul>

<p>
Starting a program by means of SGE usually involves the creation
of a <b>SGE job script</b> first, which takes care of setting up
the proper environment, possibly copying some files, querying
the nodes that are used, and starting the actual processes
on the compute nodes that were reserved for the run.
An MPI-based example can be found below.

</p><p>
The SGE system should be the only way in which processes on the DAS compute
nodes are invoked; it provides exclusive access to the reserved processors.
This is vitally important for doing controlled performance experiments,
which is one of the main uses of DAS-4.

</p><p>
NOTE: People circumventing the reservation system will risk their account
being blocked!

</p><h2>Temporary files in /local</h2>

<p>
If variable SGE_KEEP_TMPFILES is set to "no" in "$HOME/.bashrc",
at the end of the job any files created by the user in the /local
file system will be removed by SGE.
</p><p>
If SGE_KEEP_TMPFILES is set to "yes", the user's files in /local will
be left untouched.
</p><p>
If SGE_KEEP_TMPFILES is not set (as is the default), only the user's JavaGAT
sandbox directories named /local/.JavaGAT_SANDBOX.* will be deleted.

</p><h2>Prun user interface</h2>

<p>
An alternative, often more convenient way to use SGE is via the
<a href="http://www.cs.vu.nl/das4/prun.shtml">prun</a> user interface.
The advantage is that the <em>prun</em> command acts as a synchronous,
shell-like interface, which was originally developed for DAS.  
For DAS-4, the user interface is kept the same,
but the node reservation is done by SGE.
Therefore, SGE- and prun-initiated jobs don't interfere with each other.
Note that on DAS-4 a <tt>module</tt> command,
as for SGE mentioned above, should be used before invoking prun.
See also the manual pages for
<a href="http://www.cs.vu.nl/das4/preserve.shtml">preserve</a>.

</p><h2>SGE caveats</h2>

<ul>
<li>
SGE does not <b>enforce</b> exclusive access to the reserved processors.
It is not difficult to run processes on the compute nodes behind
the reservation system's back. However, this harms your fellow users, and
yourself when you are interested in performance information.
</li>
<li>
SGE does not alter the users' environment.
In particular this means, that pre-set execution limits
(such as memory_use, cpu_time, etc) are not changed.
We think this is the way it ought to be:
if users want to change their environment, they should do so
with the appropriate "ulimit" command in their .bashrc (for bash users)
or .cshrc (for csh/tcsh users) files in their home directory.
</li>
<li>
NOTE: your .bash_profile (for bash users) or .login (for csh/tcsh users)
is <b>NOT</b> executed within the SGE job, so be very careful with
environment settings in your .bashrc/.cshrc.
</li>
</ul>

<hr>
<h2>SGE/MPI example</h2>

<p>
Here, we will discuss a simple parallel example application,
using MPI on a single DAS-4 cluster.
<!--- See the <a href="openmpi-tcp.shtml">DAS-3 OpenMPI</a> page
for an example regarding multi-cluster Grid runs using OpenMPI. -->
</p><ul>
<li>Step 1: Inspect the source code:</li>
</ul>
<hr>
<pre>$ cat cpi.c
#include "mpi.h"
#include &lt;stdio.h&gt;
#include &lt;math.h&gt;

static double
f(double a)
{
    return (4.0 / (1.0 + a*a));
}

int
main(int argc, char *argv[])
{
    int done = 0, n, myid, numprocs, i;
    double PI25DT = 3.141592653589793238462643;
    double mypi, pi, h, sum, x;
    double startwtime = 0.0, endwtime;
    int  namelen;
    char processor_name[MPI_MAX_PROCESSOR_NAME];

    MPI_Init(&amp;argc, &amp;argv);
    MPI_Comm_size(MPI_COMM_WORLD, &amp;numprocs);
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;myid);
    MPI_Get_processor_name(processor_name, &amp;namelen);

    fprintf(stderr, "Process %d on %s\n", myid, processor_name);

    n = 0;
    while (!done) {
        if (myid == 0) {
	    if (n == 0) {
		n = 100; /* precision for first iteration */
	    } else {
		n = 0;   /* second iteration: force stop */
	    }

	    startwtime = MPI_Wtime();
        }

        MPI_Bcast(&amp;n, 1, MPI_INT, 0, MPI_COMM_WORLD);
        if (n == 0) {
            done = 1;
        } else {
            h   = 1.0 / (double) n;
            sum = 0.0;
            for (i = myid + 1; i &lt;= n; i += numprocs) {
                x = h * ((double) i - 0.5);
                sum += f(x);
            }
            mypi = h * sum;

            MPI_Reduce(&amp;mypi, &amp; pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

            if (myid == 0) {
                printf("pi is approximately %.16f, error is %.16f\n",
                       pi, fabs(pi - PI25DT));
		endwtime = MPI_Wtime();
		printf("wall clock time = %f\n", endwtime - startwtime);
	    }
        }
    }

    MPI_Finalize();

    return 0;
}
</pre>
<hr>

<br>
<ul>
<li>Step 2: Compile the code with OpenMPI:</li>
</ul>
<hr>
<pre>$ module load openmpi/gcc
$ which mpicc
/cm/shared/apps/openmpi/gcc/64/1.4.2/bin/mpicc
$ mpicc -O2 -o cpi cpi.c
</pre>
<hr>

<br>
<ul>
<li>Step 3: Adapt the SGE job submission script to your needs. In general,
running a new program using SGE only requires a few minor
changes to an existing job script.  A job script can be any regular
shell script, but a few SGE-specific annotations in comments
starting with "#$" are used to influence scheduling behavior.
A script to start an MPI (OpenMPI) application could look like this:</li>
</ul>
<hr>
<pre>$ cat cpi.job
#!/bin/bash
#$ -pe openmpi 16
#$ -l h_rt=0:15:00
#$ -N CPI
#$ -cwd

APP=./cpi
ARGS=""

# Get OpenMPI settings
. /etc/bashrc
module load openmpi/gcc

# Make new hostfile specifying the cores per node wanted
ncores=8
HOSTFILE=$TMPDIR/hosts
for host in `uniq $TMPDIR/machines`; do
    echo $host slots=$ncores
done &gt; $HOSTFILE
nhosts=`wc -l &lt; $HOSTFILE`
totcores=`expr $nhosts \* $ncores`

# Use regular ssh-based startup instead of OpenMPI/SGE native one
unset PE_HOSTFILE
PATH=/usr/bin:$PATH

$MPI_RUN -np $totcores --hostfile $HOSTFILE $APP $ARGS
</pre>
<hr>

<p>
In this example, on the first line we specified the shell
"/bin/bash" to be used for the execute of the job script.

</p><p>
The statement "-pe openmpi 16" asks for a "parallel environment"
called "openmpi", to be run with 2 nodes.  We need to specify 16
since every regular compute node on DAS-4 has 8 cores, corresponding
to 8 SGE "job slots", and 2 times 8 is 16.
The parallel environment specification causes SGE to use a wrapper
script that knows about the SGE job interfaces, and which can be
used to conveniently start parallel applications of a particular kind,
in this case OpenMPI.

</p><p>
Next we set the job length parameters.  Here the job is given 15 minutes
walltime maximum; if it takes longer than that, it will automatically
be terminated by SGE.  This is important since during working hours
by default only relatively short jobs are allowed on DAS-4.

</p><p>
The name of the SGE job is then explicitly set to "CPI"
with "-N CPI"; otherwise the name of the SGE script itself
would be used (i.e., "cpi.job").  The name of the job also
determines how the job's output files will be called.

</p><p>
The "-cwd" line requests execution of the script from the
current working directory at submission time; without this,
the job would be started from the user's home directory.

</p><p>
The script then sets the program to be run (APP) and arguments to
be passed (ARGS).  The actual starting of the application is by
means of OpenMPI's "mpirun" tool.
The example job script starts one process for each processor core
on each node.  The number of cores per node may depend
on the user's preferences, if processes themselves are internally
multithreaded, etc.
To use only one MPI process per node, set "ncores" to 1.

</p><p>
<b>Note:</b> the mpirun command used should typically match
the MPI version that was used during compilation, since the
startup procedure will typically be different between the
various versions of MPI supported on DAS-4.
To select the right version, we do not use an absolute
path but use the "module" command to import the right OpenMPI
environment, after which the MPI_RUN shell variable can be used.
<br>
<br>

</p><ul>
<li>Step 4: Check to see how many nodes there are, and their
availability using qstat -f, or use "preserve -llist" to quickly
check the current node usage:</li>
</ul>
<hr>
<pre>$ qstat
job-ID  prior   name       user         state submit/start at     queue                          slots ja-task-ID 
-----------------------------------------------------------------------------------------------------------------
   4671 0.50617 prun-job   versto       r     11/18/2010 23:15:21 all.q@node001.cm.cluster          32        

$ qstat -f
queuename                      qtype resv/used/tot. load_avg arch          states
---------------------------------------------------------------------------------
all.q@node001.cm.cluster       BIP   0/8/8          0.00     lx26-amd64    
   4671 0.50617 prun-job   versto       r     11/18/2010 23:15:21     8        
---------------------------------------------------------------------------------
all.q@node002.cm.cluster       BIP   0/8/8          0.00     lx26-amd64    
   4671 0.50617 prun-job   versto       r     11/18/2010 23:15:21     8        
---------------------------------------------------------------------------------
all.q@node003.cm.cluster       BIP   0/8/8          0.00     lx26-amd64    
   4671 0.50617 prun-job   versto       r     11/18/2010 23:15:21     8        
---------------------------------------------------------------------------------
all.q@node004.cm.cluster       BIP   0/8/8          0.00     lx26-amd64    
   4671 0.50617 prun-job   versto       r     11/18/2010 23:15:21     8        
---------------------------------------------------------------------------------
all.q@node005.cm.cluster       BIP   0/8/8          0.00     lx26-amd64    
---------------------------------------------------------------------------------
all.q@node006.cm.cluster       BIP   0/0/8          0.01     lx26-amd64    
[..]

$ preserve -llist
Thu Nov 18 23:16:15 2010

id   user   start       stop        state nhosts hosts
4671 versto 11/18 23:15 11/18 23:19 r     4      node001 node002 node003 node004
</pre>
<hr>
<p>
Note: DAS-4 compute nodes will generally be divided over two queues
called "all.q" (containing standard nodes) and "gpu.q" respectively
(similar nodes, but with a GPU).
Depending on the site, additional nodes for special purposes may
be available as well (e.g., for jobs requiring more memory,
more or faster cores, or more local storage), but these will be
put in a queue called "fat.q" requiring special SGE parameters
to prevent their accidental use.

</p><ul>
<li>Step 5: Submit the SGE job and check its status until it has completed:</li>
</ul>
<hr>
<pre>$ module load sge
$ qsub cpi.job
Your job 4675 ("CPI") has been submitted
$ qstat -u $USER
job-ID  prior   name       user         state submit/start at     queue                          slots ja-task-ID 
-----------------------------------------------------------------------------------------------------------------
   4675 0.45617 CPI        versto       r     11/18/2010 23:38:51 all.q@node043.cm.cluster          16        

$ preserve -llist
Thu Nov 18 23:39:18 2010

id   user   start       stop        staten hosts hosts
4675 versto 11/18 23:38 11/18 23:53 r      2     node010 node043

$ preserve -llist
Thu Nov 18 23:45:33 2010

$
</pre>
<hr>

<p>
Note that besides the consise "preserve -llist" output,
there are many ways to get specific information about the
running jobs using SGE's native qstat command.
To get an overview of the options available, run "qstat -help".

<br>
</p><ul>
<li>Step 6: Examine the standard output and standard error files for
job with ID 4675:</li>
</ul>
<hr>
<pre>[versto@fs0 cpi]$ cat CPI.o4675 
pi is approximately 3.1416009869231249, error is 0.0000083333333318
wall clock time = 0.050803
[versto@fs0 cpi]$ cat CPI.e4675 | sort -n -k 2
Process 0 on node043
Process 1 on node043
Process 2 on node043
Process 3 on node043
Process 4 on node043
Process 5 on node043
Process 6 on node043
Process 7 on node043
Process 8 on node010
Process 9 on node010
Process 10 on node010
Process 11 on node010
Process 12 on node010
Process 13 on node010
Process 14 on node010
Process 15 on node010
</pre>
<hr>

<h2>Prun/MPI example</h2>

<p>
Using the Prun user interface on top of SGE can often be more convenient
as the following examples show.

</p><p>
The number of compute nodes is specified by the "-np <i>nodes</i>" argument.
By default one process per node is started.
To specify more processes per node, use the argument "-<i>nprocs</i>".

</p><hr>
<pre>$ module load prun

$ prun -v -np 2 -sge-script $PRUN_ETC/prun-openmpi `pwd`/cpi
Reservation number 4679: Reserved 2 hosts for 900 seconds 
Run on 2 hosts for 960 seconds from Fri Nov 19 00:04:51 2010
: node043/0 node010/0 
Process 0 on node043
Process 1 on node010
pi is approximately 3.1416009869231241, error is 0.0000083333333309
wall clock time = 0.018761

$ prun -v -4 -np 2 -sge-script $PRUN_ETC/prun-openmpi `pwd`/cpi
Reservation number 4680: Reserved 2 hosts for 900 seconds 
Run on 2 hosts for 960 seconds from Fri Nov 19 00:06:22 2010
: node026/0 node026/1 node026/2 node026/3 node044/0 node044/1 node044/2 node044/3 
Process 0 on node026
Process 1 on node026
Process 2 on node026
Process 3 on node026
Process 4 on node044
Process 5 on node044
Process 6 on node044
Process 7 on node044
pi is approximately 3.1416009869231249, error is 0.0000083333333318
wall clock time = 0.036693
</pre>
<hr>

<p>
Here, the generic Prun/SGE script <em>$PRUN_ETC/prun-openmpi</em>
is used to start the OpenMPI application, similar to the SGE example above.
The script also uses a number of environment variables that are
provided by Prun:
</p><hr>
<pre>$ cat $PRUN_ETC/prun-openmpi
#!/bin/sh

# Construct host file for OpenMPI's mpirun:
NODEFILE=$TMPDIR/hosts

# Configure specified number of CPUs per node:
( for i in $PRUN_PE_HOSTS; do
    echo $i slots=$PRUN_CPUS_PER_NODE
  done
) &gt; $NODEFILE

# Need to disable SGE's PE_HOSTFILE, or OpenMPI will use it instead or the
# constructed nodefile based on prun's info:
unset PE_HOSTFILE

module load openmpi/gcc

$MPI_RUN $OMPI_OPTS --hostfile $NODEFILE $PRUN_PROG $PRUN_PROGARGS
</pre>
<hr>
<p>
Note though, that SGE "#$" directives are ignored by prun when using a job script.

</p><p>
Environment variable OMPI_OPTS is passed to the MPI deployment
tool "mpirun", which can be used to provide MPI-specific runtime options.
For example, this can be used to enforce MPI process to CPU-socket binding
(sometimes useful to improve performance due to the NUMA architecture):
</p><hr>
<pre>$ prun OMPI_OPTS="--bind-to-socket --bysocket" -sge-script $PRUN_ETC/prun-openmpi etc
</pre>
<hr>

<p>
To use TCP/IP as a network protocol using the IP-over-InfiniBand implementation instead of the low-level InfiniBand driver:
</p><hr>
<pre>$ prun OMPI_OPTS="--mca btl tcp,self --mca btl_tcp_if_include ib0" -sge-script $PRUN_ETC/prun-openmpi etc
</pre>
<hr>

<p>
For more details, see the <a href="http://www.cs.vu.nl/das4/prun.shtml">prun manual page</a>
and the <a href="http://www.open-mpi.org/">OpenMPI documentation</a>.

    </p></div>
</div>

        <div id="Footer">
            <div id="Copy">
                <p>Copyright © 2012 Vrije Universiteit Amsterdam</p>
            </div>
        </div>
    </div>
</div>

<script type="text/javascript">
<!--
var MenuBar1 = new Spry.Widget.MenuBar("MainMenu", {imgDown:"SpryAssets/SpryMenuBarDownHover.gif", imgRight:"SpryAssets/SpryMenuBarRightHover.gif"});
//-->
</script>

<!-- Start of StatCounter Code -->
<script type="text/javascript" language="javascript">
var sc_project=1574812; 
var sc_invisible=1; 
var sc_partition=14; 
var sc_security="4994de56"; 
var sc_text=4; 
</script>

<script type="text/javascript" language="javascript" src="./DAS-4  Distributed ASCI Supercomputer 4_files/counter.js"></script><noscript>&lt;a href="http://www.statcounter.com/" target="_blank"&gt;&lt;img  src="http://c15.statcounter.com/counter.php?sc_project=1574812&amp;amp;java=0&amp;amp;security=4994de56&amp;amp;invisible=0" alt="free html hit counter" border="0"&gt;&lt;/a&gt; </noscript>
<!-- End of StatCounter Code -->





</body></html>