\documentclass[a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{listings}
\usepackage{multicol}
\lstset{language=c,frame=single,captionpos=b}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage[backend=biber, sorting=none,maxbibnames=40]{biblatex}
\renewbibmacro{in:}{}
\addbibresource{ref.bib}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage[margin=2.5cm]{geometry}
\usepackage{subcaption}
\usepackage[affil-it]{authblk}
\usepackage{color}
\usepackage{amssymb,amsmath}
\usepackage{subfloat}
\usepackage{float}

\begin{document}
\title{MPI Assignment: N-body Simulations}
\author{Stefano Sandon√†}
\affil{Vrije Universiteit Amsterdam, Holland}
\date{}
		
\maketitle

\section{The N-body problem}
\label{sec:nbody_problem}
The N-body problem consists of the prediction of the individual motions of N celestial objects (bodies) with some properties (mass, initial velocity, radius, ...). This is done by measuring the force that they exert on each other (Coulomb gravity,...) and as a result, a simulation of the behavior of the system over time is obtained.  

\section{The sequential algorithm}
\label{sec:seq_algo}
The Listing \ref{simple} shows the simplified structure of the sequential algorithm.
\begin{lstlisting}[label=simple, caption=general sequential algorithm]
for each timestep do
	Compute forces between all bodies
	Compute new positions and velocities
\end{lstlisting}

As first thing, the time is discretized, so that the application knows what to do at certain time steps. After that, for each step, the algorithm computes the force between the bodies. With the result, the new velocity and positions of all the bodies are updated.

\subsection{Force computation}
\label{sec:force_comp_seq}
This step of the algorithm is the most expensive and define its complexity.
The force that bodies exert on each other has to be calculated once per pair of bodies (Listing \ref{forceloop}).
\begin{lstlisting}[label=forceloop, caption=loop for the force calculation]
for (b=0; b<bodyCt; ++b) {
		for (c=b+1; c<bodyCt; ++c) {
			...
\end{lstlisting}
With N bodies, there are (N-1)+(N-2)+(N-3)+...+1 pairs, so that O(N\textsuperscript{2}) force computations for every time step.

\section{Parallel N-body algorithm}
\label{sec:par}
An efficient parallelization of this algorithm is not trivial because for every step the updated information from all the bodies that are part of our system is needed in order to calculate the correct velocities and positions. The next paragraph was introduced to explain the technology used to develop the parallel implementation.

\subsection{MPI}
\label{sec:mpi}
For this project was used the Message Passing Interface (MPI), a message passing library that is developed and maintained by a consortium of academic, research, and industry partners, that can be added to sequential languages (C, Fortran) to program multiple nodes. Using this library, to set up a point-to-point or collective communication is simplifyed. In addition to High performance, network and process fault tolerance is provided, making this library optimal for the parallel programming.

\subsection{Work distribution}
\label{sec:bodies_distr}
One of the major problems of the parallel algorithms is the load imbalance. If the work is not fairly distributed among the machines involved in the computation, there will be some machines idle while the others will still work. This problem affects a lot the performance of the applications because the overall execution time depends on the last machine that terminate the computation. 
Looking at this specific case, there where two possible ways to follow: a fairly distribution of the bodies or a fairly distribution of the forces to compute. The major differences between this 2 approaches were encountered when the amount of work to distribute was not perfectly divisible by the number of machines.
Using a simple \textit{for} construct (Listing \ref{fair}), it is possible to calculate the chunks of forces/bodies to assign to each computational node. The Figure \ref{fig:repa} shows two examples of repartition.

\begin{lstlisting}[label=fair, caption=fair distribution of work]
int sum = 0;
int rem = workAmount % numProcs; 
for (i = 0; i < numprocs; i++) {
    work_per_proc[i] = workAmount / numProcs;
    if (rem > 0) {
        work_per_proc[i]++;
        rem--;
    }
    displs[i] = sum;
    sum += work_per_proc[i];
}
\end{lstlisting}
\FloatBarrier

\begin{figure*}[!ht]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{array_procs_3}
        \caption{3 computational nodes}
        \label{fig:repa1}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{array_procs_5}
        \caption{5 computational nodes}
        \label{fig:repa2}
    \end{subfigure}
    \caption{Repartition of 12 forces/bodies}
    \label{fig:repa}
    	
\end{figure*}
\FloatBarrier

A comparison of the two approaches was reported on the next paragraphs in order to clarify the made decisions.

\subsection{Approach 1 - Bodies repartition}
\label{sec:a1}

The total amount of bodies (known at the beginning) is distributed as fairly as possible among the  MPI processes as shown in the Listing \ref{fair}. Then, to avoid a duplication of work, given a pair of bodies chunks, half of the forces are calculated by one MPI node, and half by the other. The ripartition is clarify in the Figures \ref{fig:pr1}, \ref{fig:pr2} and \ref{fig:pr3}.


\begin{figure}[ht]
\centering
\begin{subfigure}{.35\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{array_proc_0_b_0}
\end{subfigure}%
\begin{subfigure}{.35\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{array_proc_0_b_1}
\end{subfigure}\\ %
\begin{subfigure}{.35\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{array_proc_0_b_2}
\end{subfigure}
  \caption{Forces computed by Node 0 (6+8+8=22)}
  \label{fig:pr1}
\end{figure}
\FloatBarrier


\begin{figure}[ht]
\centering
\begin{subfigure}{.35\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{array_proc_1_b_0}
\end{subfigure}%
\begin{subfigure}{.35\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{array_proc_1_b_1}
\end{subfigure}\\ %
\begin{subfigure}{.35\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{array_proc_1_b_2}
\end{subfigure}
  \caption{Forces computed by Node 1 (8+6+8=22)}
  \label{fig:pr2}
\end{figure}
\FloatBarrier


\begin{figure}[ht]
\centering
\begin{subfigure}{.35\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{array_proc_2_b_0}
\end{subfigure}%
\begin{subfigure}{.35\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{array_proc_2_b_1}
\end{subfigure}\\ %
\begin{subfigure}{.35\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{array_proc_2_b_2}
\end{subfigure}
  \caption{Forces computed by Node 2 (8+8+6=22)}
  \label{fig:pr3}
\end{figure}
\FloatBarrier

Proceeding in this way, the forces between all pairs of bodies are calculated only one time per step. Each node computes the forces of 22 pairs. There are 3 nodes, so 3 times 22 is 66 that correspond to the number of possible pairs. No useful work is done. The approximated computational time is shown in the Equation \ref{eq:eq2}. The first O represents the computation of the forces between the assigned chunk's bodies and the bodies of other chunks, while the second O represents the computation of the forces among the assigned chunk's bodies.

\begin{equation} \label{eq:eq2}
\begin{split}
Complexity & = O(\lceil\frac{N}{P}\rceil * (\lceil\frac{N}{P}\rceil * \frac{1}{2}) * (P-1)) +  O(\sum_{i=1}^{\lceil\frac{N}{P}\rceil} {(\lceil\frac{N}{P}\rceil-i)})
\end{split}
\end{equation}

\subsection{Approach 2 - Forces repartition}
\label{sec:a2}
With this approach, the bodies are not taken into consideration, but only the forces to compute, resulting in a more simple and intuitive way to proceed. After calculating the total amount of forces to compute per step (Listing \ref{forces}), this can be simply distributes as shown in Listing \ref{fair}. The approximated computational time is shown in the Equation \ref{eq:eq3}.

\begin{lstlisting}[label=forces, caption=total amount of forces]
int forceCt = 0;
    for(i = 0; i < bodyCt; i++) {
        forceCt += i;
    }
\end{lstlisting}

\begin{equation} \label{eq:eq3}
\begin{split}
Complexity & = O(\lceil\frac{N^2}{P}\rceil)
\end{split}
\end{equation}

\subsection{Solution Comparison - Load imbalance}
\label{sec:b_rep}
As mentioned before, the load imbalance is a painfull aspect of the parallel programming. If the total amount of work is perfectly divisible by the number of MPI processes, the two approaches are not so different, in fact, the number of forces calculated per machine is the same. The problems come out, when the work (bodies or forces) is not equally distributed. With the second approach, the inequity is simply reduced at the minimum. The maximum work difference between 2 nodes is of one force, that means less than 1\% of load imbalance if the assigned forces per machine are more than 100. To reach this bound with 16 MPI nodes for example, 58 bodies (1653 forces) are sufficient. With the First approach is more difficult to balance the work, because the assigned number of bodies is different, and from one body the forces that exerts on all the others has to be calculated, that means a load imbalance of \textit{O(N)}. For this reason the second approach appeared more simple and efficient, so that it is adopted.


\subsection{The final solution}
\label{sec:comm_comp}

\subsubsection{Communication VS Computation}
\label{sec:comm_comp}
The communication is a common bottleneck of lots of parallel implementations. The data exchange between nodes should be reduced at the minimum in order to obtain good performances. The problem with the N body algorithm is that at every step the updated information (positions, velocities) of all other bodies of the system is needed in order to correctly compute the forces of the assigned chunk.. To gain performances, a good compromise between the data exchange and the work to do on each node has to be found. In particular, the ratio \textit{computation/communication} should be maximized. 

\subsubsection{Approach one}
\label{sec:app_1}
Due to the fact that the force computation is the most expensive part of the algorithm and that as less data as possible should be exchanged, a good compromise is the following. At the beginning, the bodies are broadcasted to all the nodes (MPI\_Bcast Figure \ref{fig:A2}). After that, at each step, each node calculates the assigned forces, the result is an array of N forces (one per body), then with an MPI\_Allreduce operation, all the calculated forces are summed (Figure \ref{fig:C2}) and as the last thing the velocities and positions for all the bodies are computed. Proceeding in this way, only an array of N elements containing the calculated forces is exchanged with the other nodes. The major problem with this approach, is that the positions and velocities of all the bodies are calculated in every node, that represents a duplication of work. An approximation of the whole complexity and of the total amount of data exchanged per step is presented in Equations \ref{eq:comp_app1} and \ref{eq:com_app1}. In the first equation, the first \textit{O} represents the force computation step, the second the velocities computation and the third the positions computation.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\linewidth]{broadcast2}
  \caption{Broadcast the N bodies to all the nodes}
  \label{fig:A2}
\end{figure}
\FloatBarrier


\begin{figure}[ht]
  \includegraphics[width=0.5\linewidth]{MPI_all_reduce}
  \captionof{figure}{The N forces reduction}
  \label{fig:C2}
\end{figure}
\FloatBarrier

\newpage

\begin{equation} \label{eq:comp_app1}
\begin{split}
Complexity (per step) & = O(\lceil\frac{N^2}{P}\rceil)) + O(N) + O(N)\\
 & = O(\lceil\frac{N^2}{P}\rceil)
\end{split}
\end{equation}

\begin{equation} \label{eq:com_app1}
\begin{split}
Communication (per step) & = O(N)*sizeof(double)
\end{split}
\end{equation}

\subsubsection{Approach two}
\label{sec:app_1}
An alternative to approach one, is to mix the two approaches explained in Sections \ref{sec:a1} and \ref{sec:a2}. The force calculation, is the same as approach one (the one explained in Section \ref{sec:a2}), but to avoid the duplication of work computing the new velocities and positions, the idea of the bodies distribution could be expoited. The bodies could be distributed as fairly as possible (Listing \ref{fair}) and than calculate in each node only the velocities and the positions of the bodies of the assigned chunk. At the beginning, the bodies are broadcasted to all the nodes (MPI\_Bcast Figure \ref{fig:A2}). After that, at each step, each node collect the updated bodies from other nodes (Figure \ref{fig:B1}), calculates the assigned forces, the result is an array of N forces (one per body), then with an MPI\_Allreduce operation, all the calculated forces are summed (Figure \ref{fig:C2}) and as the last thing the velocities and positions for the assigned bodies are computed.
An approximation of the whole complexity and of the total amount of data exchanged per step is presented in Equations \ref{eq:comp_app2} and \ref{eq:com_app2}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\linewidth]{scatter}
  \caption{A - Scatter the bodies to the nodes}
  \label{fig:A1}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\linewidth]{MPI_all_gather}
  \caption{B - Collect updated bodies from other nodes}
  \label{fig:B1}
\end{figure}
\FloatBarrier


\begin{equation} \label{eq:comp_app2}
\begin{split}
Complexity (per step) & = O(\lceil\frac{N^2}{2P}\rceil)) + O(\lceil\frac{N}{P}\rceil) + O(\lceil\frac{N}{P}\rceil)\\
 & = O(\lceil\frac{N^2}{2P}\rceil)
\end{split}
\end{equation}

\begin{equation} \label{eq:com_app2}
\begin{split}
Communication (per step) & = O(N)*sizeof(bodyType)
\end{split}
\end{equation}


\subsubsection{Solutions comparison}
\label{sec:sol_comp}
The decision at this point, is between computing the N velocities and positions in all the nodes or exchange more data with the other nodes. 
After a comparison of the two solutions for different size of inputs (steps and number of bodies), it's clear that the amount of data exchanged is the key point. In Tables \ref{tab:t_app1} and \ref{tab:t_app2} (corresponding graphs in Figure \ref{fig:G1}) and in Tables \ref{tab:t_app3} and \ref{tab:t_app4} (corresponding graphs in Figure \ref{fig:G2}) we can see execution times and speedups of the 2 approaches for different problem sizes. It's easy to understand that the first approach present lower execution times than the other and so better speedups. That means that is more convenient to duplicate a bit the work instead of exchange more data with the other nodes. The first approach so is better.
\\


\begin{minipage}[b]{.40\textwidth}
  \centering
  \begin{tabular}{l|l|l}
  \centering
nodes\textbackslash approach & 1 & 2 \\ \hline
1 & 107.856 & 107.856 \\ \hline
2 & 63.612 & 83.450 \\ \hline
4 & 33.939 & 45.068 \\ \hline
8 & 19.566 & 26.229 \\ \hline
10 & 16.916 & 23.593 \\ \hline
16 & 12.584 & 17.523 \\ 
    \hline
  \end{tabular}
  \captionof{table}{128 bodies and 100000 iterations - Execution times}
  \label{tab:t_app1}
\end{minipage} \qquad
\begin{minipage}[b]{.40\textwidth}
  \centering
  \begin{tabular}{l|l|l}
nodes\textbackslash approach & 1 & 2 \\ \hline
2 & 1.69 & 1.29 \\ \hline
4 & 3.17 & 2.39 \\ \hline
8 & 5.51 & 4.11 \\ \hline
10 & 6.37 & 4.57 \\ \hline
16 & 8.57 & 6.15 \\ 
  \hline
  \end{tabular}
  \captionof{table}{128 bodies and 100000 iterations - Speedups}
  \label{tab:t_app2}
\end{minipage}

\begin{figure}[ht]
\begin{subfigure}{.55\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{results/graph1}
\end{subfigure} %
\begin{subfigure}{.35\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{results/graph1_sp}
\end{subfigure} 
  \caption{Execution Time and Speedups of the two approaches with 128 bodies and 100000 iterations}
  \label{fig:G1}
\end{figure}
\FloatBarrier


\enspace

\begin{minipage}[b]{.40\textwidth}
  \centering
  \begin{tabular}{l|l|l}
  \centering
nodes\textbackslash approach & 1 & 2 \\ \hline
1 & 655.162 & 655.162 \\ \hline
2 & 373.328 & 481.161 \\ \hline
4 & 186.764 & 240.538 \\ \hline
8 & 93.483 & 120.394 \\ \hline
10 & 74.908 & 96.431 \\ \hline
16 & 47.192 & 60.563 \\ 
    \hline
  \end{tabular}
  \captionof{table}{10000 bodies and 100 iterations - Execution times}
  \label{tab:t_app3}
\end{minipage} \qquad
\begin{minipage}[b]{.40\textwidth}
  \centering
  \begin{tabular}{l|l|l}
nodes\textbackslash approach & 1 & 2 \\ \hline
2 & 1.75 & 1.36 \\ \hline
4 & 3.5 & 2.72 \\ \hline
8 & 7 & 5.44 \\ \hline
10 & 8.74 & 6.79 \\ \hline
16 & 13.88 & 10.81 \\ 
  \hline
  \end{tabular}
  \captionof{table}{10000 bodies and 100 iterations - Speedups}
  \label{tab:t_app4}
\end{minipage}


\begin{figure}[ht]
\begin{subfigure}{.55\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{results/graph2}
\end{subfigure} %
\begin{subfigure}{.35\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{results/graph2_sp}
\end{subfigure}
  \caption{Execution Time and Speedups of the two approaches with 10000 bodies and 100 iterations}
  \label{fig:G2}
\end{figure}
\FloatBarrier


\subsubsection{Results}
\label{sec:res}
In this section the most interesting results are reported. In Figure \ref{fig:R1} and Tables \ref{tab:R1_t1}, \ref{tab:R1_t2} we have an example with a big number of iterations (fixed) and different numbers of bodies (small). In Figure \ref{fig:R2} and Tables \ref{tab:R2_t1}, \ref{tab:R2_t2} we have an example with a small number of iterations (fixed) and different numbers of bodies (big). As we can see, the more we increase the size of the problem (number of bodies) and the more the implementation obtains good speedups. The aim of parallel programming is to deal with big size problems, so this implementation is good due to the fact that the more we increase the number of bodies, the more the speedup is close to be linear. \\

\begin{figure}[ht]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{results/graph3}
\end{subfigure} %
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{results/graph4}
\end{subfigure} \\ %
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{results/graph5}
\end{subfigure} %
\begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{results/graph5_sp}
\end{subfigure}
  \caption{Execution Time and Speedups, 100000 iterations}
  \label{fig:R1}
\end{figure}
\FloatBarrier

\enspace

\begin{minipage}[b]{.40\textwidth}
  \centering
  \begin{tabular}{l|l|l|l}
  \centering
nodes\textbackslash bodies & 64 & 128 & 256 \\ \hline
1 & 30.414  & 120.347 & 474.974 \\ \hline
2 & 16.752 & 63.612 & 247.778 \\ \hline
4 & 9.706 & 33.939 & 127.529 \\ \hline
8 & 6.435 & 19.566 & 68.082 \\ \hline
10 & 6.155 & 16.916 & 56.687 \\ \hline
16 & 5.081 & 12.584 & 38.576 \\ 
    \hline
  \end{tabular}
  \captionof{table}{100000 iterations - Exec. times}
  \label{tab:R1_t1}
\end{minipage} \qquad
\begin{minipage}[b]{.40\textwidth}
  \centering
  \begin{tabular}{l|l|l|l}
nodes\textbackslash bodies & 64 & 128 & 256 \\ \hline
2 & 1.81 & 1.89 & 1.91 \\ \hline
4 & 3.13 & 3.54 & 3.72 \\ \hline
8 & 4.72 & 6.15 & 6.97 \\ \hline
16 & 5.98 & 9.56	 & 12.31 \\ 	\hline
  \end{tabular}
  \captionof{table}{100000 iterations - Speedups}
  \label{tab:R1_t2}
\end{minipage}

\begin{figure}[ht]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{results/graph11}
\end{subfigure} %
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{results/graph14}
\end{subfigure} \\ %
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{results/graph15}
\end{subfigure} %
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{results/graph16}
\end{subfigure} 
  \caption{Execution Time and Speedups, 100 iterations}
  \label{fig:R2}
\end{figure}
\FloatBarrier

\begin{minipage}[b]{.40\textwidth}
  \centering
  \begin{tabular}{l|l|l|l}
  \centering
nodes\textbackslash bodies & 500 & 1000 & 10000 \\ \hline
1 & 1.818 & 7.275 & 725.597 \\ \hline
2 & 0.940 & 3.749 & 373.328 \\ \hline
4 & 0.482 & 1.892 & 186.764 \\ \hline
8 & 0.273 & 1.001 & 93.483 \\ \hline
10 & 0.222 & 0.799 & 74.908 \\ \hline
16 & 0.176 & 0.555 & 47.192 \\ 
    \hline
  \end{tabular}
  \captionof{table}{100000 iterations - Exec. times}
  \label{tab:R2_t1}
\end{minipage} \qquad
\begin{minipage}[b]{.40\textwidth}
  \centering
  \begin{tabular}{l|l|l|l}
nodes\textbackslash bodies & 500 & 1000 & 10000 \\ \hline
2 & 1.93 & 1.94 & 1.94 \\ \hline
4 & 3.77 & 3.84 & 3.88 \\ \hline
8 & 6.65 & 7.26 & 7.76 \\ \hline
16 & 10.32 & 13.10 & 15.37
  \end{tabular}
  \captionof{table}{100000 iterations - Speedups}
  \label{tab:R2_t2}
\end{minipage}	


As expected, if we fix the number of bodies and we change the number of iterations, we obtain the same speedup results (Figure \ref{fig:R3} and Tables \ref{tab:R3_t1}, \ref{tab:R3_t2}). This is due to the fact that we have to repeat the same work but for different times.

\begin{figure}[ht]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{results/graph19}
\end{subfigure} %
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{results/graph17}
\end{subfigure} \\ %
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=.4\linewidth]{results/graph18}
\end{subfigure} 
  \caption{Execution Time and Speedups, 1000 bodies}
  \label{fig:R3}
\end{figure}
\FloatBarrier


\begin{minipage}[b]{.40\textwidth}
  \centering
  \begin{tabular}{l|l|l}
  \centering
nodes\textbackslash iterations & 1000 & 10000 \\ \hline
1 & 65.991 & 654.008 \\ \hline
2 & 37.488 & 374.935 \\ \hline
4 & 18.989 & 188.824 \\ \hline
8 & 9.702 & 96.724 \\ \hline
10 & 7.778 & 77.597 \\ \hline
16 & 5.070 & 50.201 \\ 
    \hline
  \end{tabular}
  \captionof{table}{1000 bodies - Execution times}
  \label{tab:R3_t1}
\end{minipage} \qquad
\begin{minipage}[b]{.40\textwidth}
  \centering
  \begin{tabular}{l|l|l}
nodes\textbackslash iterations & 1000 & 10000 \\ \hline
2 & 1.76 & 1.74 \\ \hline
4 & 3.47 & 3.46 \\ \hline
8 & 6.8 & 6.76 \\ \hline
10 & 8.48 & 8.42 \\ \hline
16 & 13.01 & 13.02 \\ 
  \hline
  \end{tabular}
  \captionof{table}{1000 bodies - Speedups}
  \label{tab:R3_t2}
\end{minipage}	

All the tests were performed on the Leiden University cluster (fs1.das4.liacs.nl) and the execution times were measured using the "wall clock time". Data distribution and data gathering (as being part of the application's initialisation and de-initialisation) are not considered on the measured performance.

\section{Conclusions and Personal considerations}
\label{sec:con}
With the realized implementation, the speedups are close to be linear for big size problems and that means that is a good implementation. This was the first time for me to use the MPI programming model and the hardest part was to understand how the MPI operations work and use them to compute various tests (e.g. MPI\_Allgatherv, MPI\_Scatterv). From my work I understood the powerful of this programming model and that is interesting to have more than one implementation in order to compare the results and outline any weaknesses.  

		
\printbibliography 

\end{document}