\documentclass[a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{listings}
\usepackage{multicol}
\lstset{language=c,frame=single,captionpos=b}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage[backend=biber, sorting=none,maxbibnames=40]{biblatex}
\renewbibmacro{in:}{}
\addbibresource{ref.bib}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage[margin=2.5cm]{geometry}
\usepackage{subcaption}
\usepackage[affil-it]{authblk}
\usepackage{color}
\usepackage{amssymb,amsmath}
\usepackage{subfloat}
\usepackage{float}

\begin{document}
\title{GPU Assignment: Image processing}
\author{Stefano Sandon√†}
\affil{Vrije Universiteit Amsterdam, Holland}
\date{}
		
\maketitle

\section{GPUs: NVIDIA GTX480}
\label{sec:nvidia}
The aim of this assignment was to learn how to use many-core accelerators, GPUs in this particular case, to parallelize data-intensive code. All the implementations were written for the \textbf{NVIDIA GTX480}, using CUDA, a parallel computing platform and programming model invented by NVIDIA. Programming with CUDA, there is a straightforward mapping onto hardware, for this reason it is necessary to study the available HW before start developing an application. The architecture of the given accelerator is is shown in Figure \ref{fig:gtx}, its main characteristics and limits are shown in Table \ref{table:t1}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{gtx}
    \caption{NVIDIA GTX480 Architecture}
    \label{fig:gtx}
\end{figure}
\FloatBarrier

\begin{table}[ht]
\centering
\begin{tabular}{l|l}
Microarchitecture & Fermi \\ \hline
Compute capability (version) & 2.0 \\ \hline
Maximum dimensionality of grid of thread blocks & 3 \\ \hline
Maximum x-dimension of a grid of thread blocks & 65535 \\ \hline
Maximum y-, or z-dimension of a grid of thread blocks & 65535 \\ \hline
Maximum dimensionality of thread block & 3 \\ \hline
Maximum x- or y-dimension of a block & 1024 \\ \hline
Maximum number of threads per block & 1024 \\ \hline
Cores per SM (warp size) & 32 \\ \hline
SM & 15 \\ \hline
Cores & 480 (32 * 15) \\ \hline
Maximum number of resident blocks per multiprocessor & 8 \\ \hline
Maximum number of resident warps per multiprocessor & 48 \\ \hline
Maximum number of resident threads per multiprocessor & 1536 (48 * 32) \\ \hline
Number of 32-bit registers per multiprocessor & 32K \\ \hline
Maximum amount of shared memory per multiprocessor & 48K \\ \hline
Theoretical Throughput & 1345 GFLOPS \\ \hline
Theoretical Bandwidth & 177.4 GB/s
\end{tabular}
\caption{NVIDIA GTX480 Specifications}
\label{table:t1}
\end{table}


\section{CImg}
\label{sec:cimg}
The image processing library used in this project was CImg, a small, modern and open-source toolkit developed for C++. CImg implements the RGB color model, an additive color model in which red, green, and blue light are added together in various ways to reproduce a broad array of colors. Each colored image of size \textit{N*M} is composed by three parts (R,G,B) of the same size, so that \textit{N*M*3} values are necessary to define an image. The Figure \ref{fig:rgb} shows an example of image composition.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{rgb}
    \caption{RGB model}
    \label{fig:rgb}
\end{figure}
\FloatBarrier

\section{The processing flow}
\label{sec:cpf}
Using CUDA there are two parts of the code: the device code, or GPU code, or the Kernel, that is a sequential program, write for one thread and execute for all and the HOST code, or CPU code, that is used to instantiate the grid, run the kernel, manage the memory. Figure \ref{fig:flow} shows the processing flow of a CUDA application. In the particular case of image processig, everything starts from the CPU, that store the image from a file into a local buffer, allocates IN and OUT buffers on the GPU (\textit{cudaMalloc}) and copy the image into the GPU's IN buffer (\textit{cudaMemCpy}). After that, the CPU launches the GPU kernel with a defined grid configuration (\textit{kernel\_function <<gridDim, blockDim>>(params)}), that is executed by the GPU following the SIMT (Single Instruction, Multiple Threads) NVIDIA model. The threads are executed in parallel in each core, and they read the assigned part of IN data and generates the assigned part of OUT data. At the end, the results are copied out back to the CPU (\textit{cudaMemCpy}) and the image is written to a file by the CPU.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{flow}
    \caption{CUDA processing flow}
    \label{fig:flow}
\end{figure}
\FloatBarrier

\section{CUDA grid configuration}
\label{sec:grid}
In CUDA, as mentioned before, there is a strinct mapping with the hardware, so that an hardware virtualization model is fixed with the concepts of thread, block and grid.
Each \textbf{thread} executes the kernel code, running on one CUDA core.
The threads are logically grouped into \textbf{thread blocks}, so that the threads of the same block will run on the same multiprocessor. The thread blocks are logically organized in a \textbf{Grid}, that represent the entire dataset. The blocks and the grid can be of 1D, 2D or 3D.
The most important thing programming with GPUs, to make use of all their power, is to make them as busy as possible. Switching between concurrent warps has no overhead because registers and share memory are partitioned and not stored/restored, so that if one warp has to wait for example for a memory access and another warp is ready, there is a switch to hide the latency. The thread scheduling is really quick, and this allow the blocks to be swapped in and out really quickly. For this reason an high number of warps is needed and instantiating a grid, is good to have a number of blocks much bigger than the number of available multiprocessor and a block size that can be higher than the amount of cuda cores available per SM.
Another interesting aspect to take into consideration, is the block size. The block is divided into warps, so that Threads 0..31 are part of Warp 0, Threads 32...63 are part of warp 1 and so on. Considering this, it is good to have blocks with a size multiple of 32, so that no useless threads are launched. After certain tests, a block of size 256 revealed to be optimal. For what concerning the grid configuration, an image is a 2D structure and for this reason it is intuitive to set up also a 2D grid. By the way, setting up a 2D grid, there is not only one way to follow. For this particular project two possible block configurations were considered, 1D (1x256), 2D (32x8), 2D (32x16) and two possible grid configurations: dynamic and fixed. In the rest part of this document, \textbf{M1} indicates a dynamic grid of 1D blocks, in which the grid has a height of \textit{image\_height}, a width of \textit{(ceil(image\_width / 256))} and the threads on the right border if the image width is not a multiple of \textit{block\_width} are idle (Figure \ref{fig:m1}); \textbf{M2} indicates a dynamic grid of 1D blocks, in which the size of the grid is the same as M1, but the threads are consecutive and only the threads on the last block(s) are idle if the image's number of pixel is not a multiple of the size of the block (Figure \ref{fig:m2}); \textbf{M3} indicates a dynamic grid of 2D blocks, in which the grid has a width of \textit{ceil(image\_width / 16)}, a height of \textit{ceil(image\_height / 16)} and the overflow is the same as M1 (Figure \ref{fig:m3}); \textbf{M4} indicates a dynamic grid of 2D blocks, in which the size of the grid is the same as M3, but the oveflow is the same as M2 (Figure \ref{fig:m4}). 

\begin{figure}[!ht]
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=\linewidth]{res/M1}
\caption{M1 - Right overflow}
\label{fig:m1}
\end{subfigure} % separation between the subfigures
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=\linewidth]{res/M2}
\caption{M2 - Overflow at the end}
\label{fig:m2}
\end{subfigure}
\caption{1D blocks, Kernel configurations, image of 16 pixels}
 \label{fig:methods12}
\end{figure}
\FloatBarrier

\begin{figure}[!ht]
 % separation between the subfigures
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=\linewidth]{res/M3}
\caption{M3 - Right overflow}
\label{fig:m3}
\end{subfigure}
 % separation between the subfigures
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=\linewidth]{res/M4}
\caption{M4 - overflow at the end}
\label{fig:m4}
\end{subfigure}
\caption{2D blocks possible kernel configurations, image of 24 pixels}
 \label{fig:methods34}
\end{figure}
\FloatBarrier

\section{Coalesced memory access}
\label{sec:cma}
One of the main bottlenecks with the GPUs are the global memory accesses that are expensive, for this reason it is better to maximize the use of bytes that travel from the DRAM to the Streaming Multiprocessor. CUDA uses a SIMT approach, in which all threads of a warp execute the same instruction, so that global memory accesses are effectuated "per warp". The threads in a warp (32) provide 32 addresses and the hardware converts these addresses into memory transactions. The memory is divided into regions of 128 bytes, so that bytes 0...127 are part of Region 0, bytes 128...255 are part of Region 1 and so on. The memory is accessed per region, that means if one thread wants the byte 0, the entire Region 0 is loaded. A kernel is correctly designed, if consecutive threads access consecutive memory addresses, so that all the requeted addresses fall on the same region and only one transaction is performed. Behaving in this "coalesced way" instead of having one access per thread, these accesses are grouped and the total memory overhead is reduced. Developing all the three algorithms this aspect was taken into consideration.

\section{Algorithm 1: Grayscale Conversion and Darkening}
\label{sec:gcd}
From an RGB image, the output of this algorithm is a darker grayscale image.
The gray value of a pixel is generated by weighting the three values (\texttt{0.3*R, 0.59*G, 0.11*B}) and then summing them together. To darken the obtained grayscale image, the final pixel value is multiplied by a constant (0.6). The Figure \ref{fig:dark} shows an example of the result. The sequential algorithm, simply go through the entire image and computes for each pixel the corresponding value (Listing \ref{dsc}).

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth]{dark}
    \caption{Grayscale Conversion and Darkening}
    \label{fig:dark}
\end{figure}
\FloatBarrier

\begin{lstlisting}[label=dsc, caption=Darker Sequential code]
//H=image_height, W=image_width
for ( int y = 0; y < H; y++ ) {
 for ( int x = 0; x < W; x++ ) {
  float grayPix = 0.0f;
  float r = static_cast< float >(inputImage[(y * W) + x]);
  float g = static_cast< float >(inputImage[(W * H) + (y * W) + x]);
  float b = static_cast< float >(inputImage[(2 * W * H) + (y * W) + x]);
  grayPix = ((0.3f * r) + (0.59f * g) + (0.11f * b));
  grayPix = (grayPix * 0.6f) + 0.5f;
  darkGrayImage[(y * W) + x] = static_cast< unsigned char >(grayPix);
 }
}
\end{lstlisting}
\FloatBarrier

\subsection{Parallelization}
\label{sec:p1}
\subsection{{One pixel per thread}}
\label{sec:dfm}
After allocating into the GPU global memory some space to contain the input image (\textit{3 * image\_width * image\_height * sizeof(unsigned char)}), copying the input image there and allocating some memory to content the output image (\textit{image\_width * image\_height * sizeof(unsigned char)}), the kernel is ready to be launched. The GPU code is the same as the code content inside the loop of the sequential version (Listing \ref{dsc}), but instead of using the indexes of the loop to access the image pixels, it uses the index associated to the thread. In this first method, only one pixel is computed per thread. To exploit the coalesced memory access, two consecutive threads computes/accesses the values of two consecutive pixels. The 4 different grid configurations (explained in Section \ref{sec:grid}) were tested for this program, in order to establish the best. In Figure \ref{fig:histo_darker} are reported the obtained results. As shown methods M1 and M2 obtained the best results, so with unidimensional blocks the algorithm achieved better speedups, probably due to the few amount of operations to calculate the index of the pixel associated to each thread. The speedups are not so different using a 1D or 2D block, because the performance for this particular task are not related to data locality. Except from the image size, that could advantage one configuration instead of another, there is no advantage using a 2D block or a 1D block if we are accessing the memory in a coalesced way.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{res/new/darker_confronto}
    \caption{Speedups Comparison}
    \label{fig:histo_darker}
\end{figure}
\FloatBarrier


\subsection{Optimization - More pixels per thread}
\label{sec:dfm}
To exploit all the power of GPUs, as said, they has to be as busy as possible, because the warp scheduling is very fast and can hide the latency. Anyway, reducing the number of thread blocks by increasing the work per thread revealed to achieve better results. Lots of experimets were performed to establish the right number of pixels to compute by each thread, all taking in account the coalesced memory access. The grid was set up with a width of \textit{ceil(width / 256)} and a height of \textit{ceil(height / pixels\_per\_thread)} (unidimensional blocks). To mantain the coalescing, one thread computes the associated first pixel index with the formula shown in Listing \ref{cfp} and then for the following pixels, it adds each time to this the total amount of pixels contained in the grid(\textit{gridDim.x * blockDim.x * gridDim.y * blockDim.y}). The final code, is the one shown in Listing \ref{dpc}. 

\begin{lstlisting}[label=cfp, caption=Corresponding first pixel]
unsigned int pixel=((blockIdx.y * gridDim.x + blockIdx.x) 
                   * blockDim.x) + threadIdx.x
\end{lstlisting}
\FloatBarrier

\begin{lstlisting}[label=dpc, caption=Darker Parallel Code]
for(i = ((blockIdx.y * gridDim.x + blockIdx.x) * blockDim.x) + threadIdx.x; 
    i < width * height; 
    i += (gridDim.x * blockDim.x) * (gridDim.y * blockDim.y)) {
float grayPix = 0.0f;
float r = static_cast< float >(inputImage[i]);
float g = static_cast< float >(inputImage[(width * height) + i]);
float b = static_cast< float >(inputImage[(2 * width * height) + i]);
grayPix = ((0.3f * r) + (0.59f * g) + (0.11f * b));
grayPix = (grayPix * 0.6f) + 0.5f;
outputDarkGrayImage[i] = static_cast< unsigned char >(grayPix);
}
\end{lstlisting}
\FloatBarrier

Figure \ref{fig:darker_try_histo} represents the speedups achieved for different thread loads. After a certain number of pixels computed per thread, the performance of the program started to decrese, for this reason the idea to use a grid with a fixed size, instead of a dynamic size was not taken into consideration. Following this approach, with a number of 10 pixels per thread, the program obtained often the best results, so this is a good thread load to help the scheduler. This load was sufficient to both hide the latency and reduce the warp scheduling overhead. Figure \ref{fig:darker_single_more_comparison} shows the comparison between the best results obtained with a single pixel per thread and the results obtained with the new version. It is clear that the new version is an optimization, in fact for each image the speeedup increased. This solution is the one adopted. In Table \ref{tab:darker_ex} and \ref{tab:darker_sp} are shown the detailed execution times and speedups achieved. Table \ref{tab:darker_t_b} exposes the Achieved Throughput and Bandwidth. Table \ref{tab:pxabd} exposes the differences obtained comparing the output of the sequential algorithm with outpur of the parallel algorithm. These differences could derive from a difference in floating-point precision between the CPU and GPU or from different ordering of floating-point operations. In Section \ref{sec:vp} is reported a detailed analysis using the NVIDIA Virtual Profiler.
    
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{res/new/darker_try_histo}
    \caption{Speedup with more different number of pixels per thread}
    \label{fig:darker_try_histo}
\end{figure}
\FloatBarrier

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{res/new/darker_single_more_comparison}
    \caption{Solutions comparison}
    \label{fig:darker_single_more_comparison}
\end{figure}
\FloatBarrier

\begin{table}[!ht]
\centering
\label{tab:darker_ex}
\begin{tabular}{|c|l|c|c|}
\hline
               & \textbf{Sequential} & \textbf{M2} & \textbf{10 pixels per thread} \\ \hline
\textbf{img00} & 0.002416            & 0.000076    & 0,000076                      \\ \hline
\textbf{img01} & 0.036925            & 0.000202    & 0,000193                      \\ \hline
\textbf{img02} & 0.033726            & 0.000279    & 0,000255                      \\ \hline
\textbf{img03} & 0.340821            & 0.001537    & 0,001299                      \\ \hline
\textbf{img04} & 0.151429            & 0.001091    & 0,000907                      \\ \hline
\textbf{img05} & 0.445976            & 0.002090    & 0,001692                      \\ \hline
\textbf{img06} & 0.588814            & 0.003880    & 0,003239                      \\ \hline
\end{tabular}
\caption{Execution Times}
\end{table}
\FloatBarrier

\begin{table}[!ht]
\centering
\label{tab:darker_sp}
\begin{tabular}{|c|c|c|}
\hline
               & \textbf{M2} & \textbf{10 pixels per thread} \\ \hline
\textbf{img00} & 31.79       & 31.79                         \\ \hline
\textbf{img01} & 182.80      & 191.32                        \\ \hline
\textbf{img02} & 120.88      & 132.26                        \\ \hline
\textbf{img03} & 221.74      & 262.37                        \\ \hline
\textbf{img04} & 138.80      & 166.96                        \\ \hline
\textbf{img05} & 213.39      & 263.58                        \\ \hline
\textbf{img06} & 151.76      & 181.79                        \\ \hline
\end{tabular}
\caption{Speedups}
\end{table}
\FloatBarrier

\begin{table}[!ht]
\centering
\label{tab:pxabd}
\begin{tabular}{|c|l|c|c|l|l|l|l|}
\hline
\textbf{}                        & \textbf{img00}           & \textbf{img01} & \textbf{img02} & \textbf{img03}           & \textbf{img04}            & \textbf{img05}         & \textbf{img06}            \\ \hline
\textbf{Pixels above  threshold} & \multicolumn{1}{c|}{111} & 231            & 135            & \multicolumn{1}{c|}{735} & \multicolumn{1}{c|}{1503} & \multicolumn{1}{c|}{9} & \multicolumn{1}{c|}{4206} \\ \hline
\end{tabular}
\caption{Differences between the Sequential and the Parallel output}
\end{table}

\begin{table}[!ht]
\centering
\label{tab:darker_t_b}
\begin{tabular}{|c|c|c|c|c|}
\hline
               & \textbf{\begin{tabular}[c]{@{}c@{}}Ach. Throughput\\ (GFLOPS/s)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Ach. Bandwidth \\ (GB/s)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Compute utilization \\ (\%)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Bandwidth utilization \\ (\%)\end{tabular}} \\ \hline
\textbf{Img00} & 24,14                                                                             & 13,80                                                                         & 1,80                                                                         & 7,78                                                                           \\ \hline
\textbf{Img01} & 95,08                                                                             & 54,33                                                                         & 7,07                                                                         & 30,63                                                                          \\ \hline
\textbf{Img02} & 101,20                                                                            & 57,83                                                                         & 7,52                                                                         & 32,60                                                                          \\ \hline
\textbf{Img03} & 136,02                                                                            & 77,72                                                                         & 10,11                                                                        & 43,81                                                                          \\ \hline
\textbf{Img04} & 129,48                                                                            & 73,99                                                                         & 9,63                                                                         & 41,71                                                                          \\ \hline
\textbf{Img05} & 137,11                                                                            & 78,35                                                                         & 10,19                                                                        & 44,16                                                                          \\ \hline
\textbf{Img06} & 141,63                                                                            & 80,93                                                                         & 10,53                                                                        & 45,62                                                                          \\ \hline
\end{tabular}
\caption{Achieved Throughput and Bandwidth}
\end{table}
\FloatBarrier


\section{Algorithm 2: Histogram Computation}
\label{sec:hc}
From an RGB image, the output of this algorithm is a grayscale image with the relative histogram of 256 possible values of gray.
The histogram measures how often a value of gray is used in an image. The sequential algorithm, simply go through the entire image, computing for each pixel the corresponding gray value and incrementing the corresponding counter (Listing \ref{histo_loop}). The Figure \ref{fig:histo} shows an example of the result.

\begin{lstlisting}[label=histo_loop, caption=Sequential code]
//H=image_height , W=image_width
for ( int y = 0; y < H; y++ ) {
 for ( int x = 0; x < W; x++ ) {
  float grayPix = 0.0f;
  float r = static_cast< float >(inputImage[(y * W) + x]);
  float g = static_cast< float >(inputImage[(W * H) + (y * W) + x]);
  float b = static_cast< float >(inputImage[(2 * W * H) + (y * W) + x]);
  grayPix = ((0.3f * r) + (0.59f * g) + (0.11f * b)) + 0.5f;
  grayImage[(y * W) + x] = static_cast< unsigned char >(grayPix);
  histogram[static_cast< unsigned int >(grayPix)] += 1;
 }
}
\end{lstlisting}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth]{histo}
    \caption{Histogram Computation}
    \label{fig:histo}
\end{figure}
\FloatBarrier

\subsection{Parallelization}
\label{sec:p2}
\subsection{One pixel per thread}
\label{sec:fm2}
After copying the input image and the initial empty histogram into the GPU global memory and allocating some memory to content the output image, the kernel is ready to be launched. As for the previous algorithm, the GPU code is the same as the code content inside the loop of the sequential version (Figure \ref{histo_loop}), but instead of using the indexes of the loop to access the image pixels, it uses the index associated to the thread. Also in this case, to exploit the coalesced memory access, two consecutive threads computes/accesses the values of two consecutive pixels. The interesting aspect of computing histograms in parallel, is that it is possible that two threads read at the same time the same value of gray for a pixel, so that at the same time they try to increment the corresponding histogram bin. For a correct execution, to avoid wrong updates, the increment has to be an atomic operation, so that only one thread at the time will modify the bin value. 
The logic behind an atomic operation is that each thread "locks" the variable, modify it, and "unlocks" it, so that the other threads that try to modify the same variable has to wait that this will be "unlocked". The worst case, in the histogram scenario, is a monochrome image (only one color), in which all the threads try to modify the same bin, which implies a long waiting queue of threads. The general idea to have high performace, is to reduce the number of conflicts at the minimum. Also in this algorithm, the speedup is not related to the shape of the block, in fact the performance of the algorithm using the atomics depends on the image. Dealing with an image with horizontal stripes of different colors of 1 pixel for example, with a 1D block (1x256) there will be all the 256 threads that try to increment the same histogram bin. In the same scenario, using a 2D block (8x32) there will be only 32 threads that try to increment the same histogram value. If the image instead, is composed by vertical stripes of 32 pixels width, it happens the opposite, the 1D approach will obtain more performance. For these reasons, for the initial tests, the given images were substitute with images of the same sizes but composed by only one color (monochrome). Different implementations were tested. The first approach consisted on using only the histogram array stored on the global memory and modifying it with atomic add operations (Figure \ref{fig:ghgh}). The second approach consisted on using an additional histogram array stored in shared memory, performing the atomic add operations there and at the end, assigning a certain bin to a thread and reducing the shared histograms in the global one (Figure \ref{fig:shsh}). Doing this, not all the launched threads will compete for the same bin, but only the threads inside the same block during the computation and the threads to which is assigned the same bin during the reduce phase. Figure \ref{fig:gavsa} shows the obtained results for the two approaches. From this image it is clear that the second approach drastically increased the speedups and that the block configuration (1D or 2D) doesn't affect the performance.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth]{global_histo}
    \caption{Global histogram - Concurrency}
    \label{fig:ghgh}
\end{figure}
\FloatBarrier

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth]{shared_histo}
    \caption{Shared histogram - Concurrency}
    \label{fig:shsh}
\end{figure}
\FloatBarrier

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth]{res/new/histogram_confronto}
    \caption{Global Atomics vs Shared Atomics (Monochrome images)}
    \label{fig:gavsa}
\end{figure}
\FloatBarrier

An additional "privatization" of the histogram could be performed. Instead of having one histogram per block, it is possible to assign one histogram per warp, so that the concurrency at the end is only among the 32 warp threads (Figure \ref{fig:whwh}) and among the threads to which is assigned the same bin during the reduce phase. Figure \ref{fig:sawvsa} shows the performance of this last method compared to the previous. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{histo_warp}
    \caption{Shared Warps histogram - Concurrency}
    \label{fig:whwh}
\end{figure}
\FloatBarrier

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth]{res/new/histogram_warp}
    \caption{Shared Atomics with Warps vs Shared Atomics (Monochrome images)}
    \label{fig:sawvsa}
\end{figure}
\FloatBarrier

After a testing phase with monochrome images, the tests were performed with the real given images. Figure \ref{fig:gasawvsa} represents the obtained results. As it is possible to see, in this case the warp solution is much worst than the shared version, that means on standard images, the time spent to reduce the single warp histograms is bigger than the time spent to compute the atomic operations. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth]{res/new/histogram_confronto_real}
    \caption{Global Atomics vs Shared Atomics vs Shared Atomics with Warps}
    \label{fig:gasawvsa}
\end{figure}
\FloatBarrier

\subsection{Optimization - More pixels per thread}
\label{sec:hop}
As for the previous algorithm, some additional tests were performed assigning the computation of more pixels per thread. The last two approaches (Shared Atomics and Shared Atomics with Warps) were improved to test this solution and both on the real images obtained better results. This is due to a reduction of the scheduling load and on the reduction of global histogram reductions. Different work loads were assigned to each thread and measured. Figures \ref{fig:hum}, \ref{fig:hfm}, \ref{fig:hwm} and \ref{fig:hwf} show the obtained results for different configurations. Figure \ref{fig:dbsc} compare the best obtained configurations for the two approaches. As shown, the second approach (Shared Atomics with Warps), differently than the one pixel per thread version, obtains for almost all the images the best results. This result could surprise, but means that the decrement on the total number of reduction operations is critical for this approach. For the smallest images (Img00) the time spent for the final reduction remains high in comparison with the total computation due to the low amount of pixels to compute. The final adopted solution is the Shared Atomics with Warps with 30pxs computed per threads, because it is good also for small images (not a big amount of useless launched blocks) and the difference with the 60x45 grid version is not so evident. Tables \ref{tab:histo_exe} and \ref{tab:histo_sp} show the detailed results.


\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.65\linewidth]{res/new/histogram_uni_more}
    \caption{Shared Atomics, 1D blocks - Different number of assigned pixels per thread}
    \label{fig:hum}
\end{figure}
\FloatBarrier

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.65\linewidth]{res/new/histogram_uni_fixed}
    \caption{Shared Atomics, 1D blocks - Fixed grid}
    \label{fig:hfm}
\end{figure}
\FloatBarrier

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.65\linewidth]{res/new/histogram_warp_more}
    \caption{Shared Atomics with Warps, 1D blocks - Different number of assigned pixels per thread}
    \label{fig:hwm}
\end{figure}
\FloatBarrier

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth]{res/new/histogram_warp_fixed}
    \caption{Shared Atomics with Warps, 1D blocks - Fixed grid}
    \label{fig:hwf}
\end{figure}
\FloatBarrier

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth]{res/new/histogram_more_confronto}
    \caption{Best solutions comparison}
    \label{fig:dbsc}
\end{figure}
\FloatBarrier

\begin{table}[!ht]
\centering
\label{tab:histo_exe}
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{}}      & \textbf{Sequential} & \textbf{30px}                 & \multicolumn{1}{c|}{\textbf{65x40}} \\ \hline
\multicolumn{1}{|c|}{\textbf{Img00}} & 0,003208            & \multicolumn{1}{c|}{0,000080} & \multicolumn{1}{c|}{0,000127}       \\ \hline
\textbf{Img01}                       & 0,032379            & 0,001264                      & 0,001062                            \\ \hline
\textbf{Img02}                       & 0,043784            & 0,001778                      & 0,001553                            \\ \hline
\textbf{Img03}                       & 0,200681            & 0,004724                      & 0,004698                            \\ \hline
\textbf{Img04}                       & 0,197973            & 0,012325                      & 0,011871                            \\ \hline
\textbf{Img05}                       & 0,267590            & 0,004278                      & 0,004260                            \\ \hline
\textbf{Img06}                       & 0,616924            & 0,010026                      & 0,009756                            \\ \hline
\end{tabular}
\caption{Execution Times}
\end{table}
\FloatBarrier

\begin{table}[!ht]
\centering
\label{tab:histo_sp}
\begin{tabular}{|l|l|l|}
\hline
\multicolumn{1}{|c|}{}               & \textbf{30px}              & \multicolumn{1}{c|}{\textbf{65x40}} \\ \hline
\multicolumn{1}{|c|}{\textbf{Img00}} & \multicolumn{1}{c|}{40.10} & \multicolumn{1}{c|}{25.26}          \\ \hline
\textbf{Img01}                       & 25.62                      & 30.49                               \\ \hline
\textbf{Img02}                       & 24.63                      & 28.19                               \\ \hline
\textbf{Img03}                       & 42.48                      & 42.72                               \\ \hline
\textbf{Img04}                       & 16.06                      & 16.68                               \\ \hline
\textbf{Img05}                       & 62.55                      & 62.81                               \\ \hline
\textbf{Img06}                       & 61.53                      & 63.24                               \\ \hline
\end{tabular}
\caption{Speedups}
\end{table}
\FloatBarrier



\begin{table}[!ht]
\centering
\caption{Differences between the Sequential and the Parallel output}
\label{pxabh}
\begin{tabular}{|c|l|c|c|l|l|l|l|}
\hline
\textbf{}                        & \textbf{img00}           & \textbf{img01} & \textbf{img02} & \textbf{img03}          & \textbf{img04}         & \textbf{img05}          & \textbf{img06}          \\ \hline
\textbf{Pixels above  threshold} & \multicolumn{1}{c|}{324} & 12             & 0              & \multicolumn{1}{c|}{24} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{48} \\ \hline
\end{tabular}
\end{table}
 

\section{Algorithm 3: Smoothing}
\label{sec:smoo}
Smoothing is the process of removing noise from an image by the means of statistical analysis. To remove the noise, each point is replaced by a weighted average of its neighbours. In this way small-scale structures are removed from the image. In this case a two-dimensional 5-point triangular smooth filter was used. The Figure \ref{fig:smooth} shows an example of the result.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{smooth}
    \caption{Smoothing}
    \label{fig:smooth}
\end{figure}
\FloatBarrier


\subsection{Parallelization}
\label{sec:p2}
This particular algorithm deals with square areas of the image (filter), so that using 2D blocks, the threads can efficiently share memory and prevent a lot of global memory accesses.

\begin{figure}[!ht]
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=\linewidth]{filter}
\caption{No square grid with 1D blocks}
\label{fig:f1}
\end{subfigure} % separation between the subfigures
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=\linewidth]{filter_letters}
\caption{Square grid with 1D blocks}
\label{fig:f2}
\end{subfigure}
\caption{Possible kernel configurations}
 \label{fig:speed}
\end{figure}
\FloatBarrier

\begin{table}[!ht]
\centering
\caption{Differences between the Sequential and the Parallel output}
\label{pxabs}
\begin{tabular}{|c|l|c|c|l|l|l|l|}
\hline
\textbf{}                        & \textbf{img00}         & \textbf{img01} & \textbf{img02} & \textbf{img03}         & \textbf{img04}         & \textbf{img05}         & \textbf{img06}         \\ \hline
\textbf{Pixels above  threshold} & \multicolumn{1}{c|}{0} & 0              & 0              & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} \\ \hline
\end{tabular}
\end{table}

\section{Algorithms analysis with the Visual Profiler}
\label{sec:vp}

Instruction-level parallelism (ILP) is a measure of how many of the operations in a computer program can be performed simultaneously. The potential overlap among instructions is called instruction level parallelism.

\subsection{Application 1}
\label{sec:a1}
After collecting the profile of the applications using \textbf{nvprof}, the output files were evaluated using the \textbf{Nvidia Visual Profiler}. The choosen configuration, uses unidimensional \textit{Blocks} of 256 threads, 12 \textit{Registers} and 0 \textit{Shared Memory}. For all the images the profiler found no issues for \textit{Divergent Execution} (threads that follow different if branches) and a \textit{Warp Execution Efficiency} (ratio of the average active threads per warp to the maximum number of threads per warp supported on a multiprocessor) of 100\%. This last value comes from the fact that is used a block of 256 threads, that is a multiple of 32 (warp size), so no useless threads are launched and from the fact that there is no thread divergent execution, so the threads within a warp can execute in a SIMD way, avoiding inactive threads within the warp. Examples of occupancy results are shown in Figure, all the the images, obtained an occupancy over 91\% except the first that obtained 82.5\%. This is probably correlated to its small size, but in any case the occupancy is not limiting the performance of the application. For all the images exept the 5th, the Profiler found no issues related to Global Memory Access Pattern. The 5th image, is the only one of the given set that has an amount of pixels that is not a multiple of 128, the memory in the device is allocated with a 128-byte line granularity, so probably, the addresses fall within 2 cache lines, so 2 transaction insteads of one and so a decrease of the memory bandwidth utilization (Figure )

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{profiling/darker/darker_occupancy_00}
    \caption{Img00}
    \label{fig:occ00}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{profiling/darker/darker_06_occupancy}
    \caption{Img05}
    \label{fig:occ06}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{profiling/darker/unaligned_load}
    \caption{Img00}
    \label{fig:unlo}
\end{figure}
\FloatBarrier

\begin{lstlisting}[label=loop, caption=Unaligned accesses]

float g = static_cast< float >(inputImage[(width * height) + (y * width) + x]);
float b = static_cast< float >(inputImage[(2 * width * height) + (y * width) + x]);

\end{lstlisting}
\FloatBarrier

 






On image number 5 => no coalesced access for the 2 instructions.. because image pixels non a multiple of 128, so non aligned (threadID+pizelsOfImage) but not effect the computation, compiling with -Xptxas -dlcm=cg worst results, Changing the configuration (cudaDeviceSetCacheConfig(cudaFuncCachePreferL1);)
2) 





\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{profiling/darker/darker_utilization_00}
    \caption{Img05}
    \label{fig:du}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{profiling/darker/darker_varying}
    \caption{Img05}
    \label{fig:dv}
\end{figure}
\FloatBarrier



\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{profiling/darker/darker_varying}
    \caption{Img05}
    \label{fig:img05Prof}
\end{figure}
\FloatBarrier

\printbibliography 

\end{document}

			 	 